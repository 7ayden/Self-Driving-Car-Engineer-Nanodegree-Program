{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer vision projects on classifying traffic sign. \n",
    "\n",
    "\n",
    ": data augmentation, why this CNN and plot the probability of the knn top5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From exporation, there are 34799 training sets and 12630 testing sets. \n",
    "Each of the image has 32*32*3.\n",
    "Roughly around 40+ classes.\n",
    "The goal is to leverage a unbalance and relativity speaking small dataset to classify with deep neuarl networking.\n",
    "\n",
    "Recall from deep learning foundations course from deeplearning.ai, classic machine learning requires around 100-10,000 samples and deep learning requires around 1 million sample. \n",
    "\n",
    "Given neural network does not require any data balance having each class, I would like to see the difference between data split and also data size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (34799, 32, 32, 3)\n",
      "X_test shape (12630, 32, 32, 3)\n",
      "Y_valid shape (12630, 32, 32, 3)\n",
      "Y_valid shape (12630,)\n",
      "Y_train shape (34799, 32, 32, 3)\n",
      "Y_test shape (12630, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "training_file = \"./traffic-signs-data/train.p\"\n",
    "validation_file = \"./traffic-signs-data/test.p\"\n",
    "testing_file = \"./traffic-signs-data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "    \n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, Y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"Y_valid shape\", X_valid.shape)\n",
    "print(\"Y_valid shape\", Y_valid.shape)\n",
    "print(\"Y_train shape\", X_train.shape)\n",
    "print(\"Y_test shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signnames = pd.read_csv('signnames.csv')\n",
    "label_names = list(signnames)\n",
    "y_train_df = pd.DataFrame()\n",
    "y_train_df['label'] = y_train\n",
    "\n",
    "# Get current size\n",
    "figsize=(15, 7)\n",
    " \n",
    "# Prints: [8.0, 6.0]\n",
    "\n",
    "item, count = np.unique(y_train, return_counts=True)\n",
    "freq = np.array((item, count)).T\n",
    "plt.figure(11)\n",
    "plt.yticks(range(len(y_train)), signnames.SignName)\n",
    "#plt.yticks(list(map(lambda x: label_dict[x], y_train['label'].value_counts().index.tolist())))            \n",
    "plt.barh(item, count, alpha=0.3)\n",
    "plt.title('Traffic Sign Data Distrubition')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot and see how data distribution looks like\n",
    "n_classes = len(np.unique(y_train))\n",
    "Y_test_class = len(np.unique(y_test))\n",
    "\n",
    "def plot_dist(data) :\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.hist(y_testing, np.arange(-0.5, n_classes+1.5), stacked=True)\n",
    "    (Y_test_class, np.arange(n_classes+1.5)).plt.bar()\n",
    "    #ax.hist(y_testing, np.arange(-0.5, n_classes+1.5), stacked=True)\n",
    "    #ax1.hist(y_training, np.arange(-0.5, n_classes+1.5), stacked=True)\n",
    "    #pd.concat(dict(df1= y_testing, df2 = y_training), azis = 1).plot(kind='bar', stack =True)\n",
    "    ax.set_xlim(-0.5,n_classes-0.5)\n",
    "    ax.legend()\n",
    "    ax.set_title('Distribution of images')\n",
    "\n",
    "plot_dist(y_train)\n",
    "plot_dist(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classID = signnames['SignName'].values\n",
    "n_classes = np.unique(y_train[0]).shape[0]\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "for i, y in enumerate(y_train):\n",
    "    ax.bar(range(n_classes), np.bincount(y), width=1., align='center', color=colors[i], label=labels[i])\n",
    "\n",
    "plt.xlim([-1, n_classes])\n",
    "ax.xaxis.grid(False)\n",
    "ax.xaxis.set_ticks_position('none')\n",
    "ax.yaxis.grid(False)\n",
    "ax.set_xticks(range(n_classes))\n",
    "ax.set_xticklabels(class_table, rotation=90, size='xx-small')\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "legend_frame = legend.get_frame()\n",
    "legend_frame.set_facecolor('white')\n",
    "legend_frame.set_edgecolor('black')\n",
    "plt.ylabel('# examples')\n",
    "plt.title(title)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the exploration, there the training and testing dataset obtains about the same ratios of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#Question 1\n",
    "Describe how you preprocessed the data. Why did you choose that technique?\n",
    "\n",
    "1. I am planning to preprocess my image by converting into greyscale that can save a lot of computer power from 3 channels to 1, since I dont have GPU on my laptop nor planning to train on cloud. \n",
    "Next step is to normalizied dataset, the process would allows each dimensions have a similar scale\n",
    "\n",
    "\n",
    "there are always different approches on this: \n",
    "1. loop image one by one with CV2 (easiest way)\n",
    "2. passing through normalizing (pixel - 128)/ 128, then img.astype(np.float32), passing through the CV2, and finally reshape from 32,32,3 to 32,32,1\n",
    "X_train_gray = np.zeros((N, 32, 32, 1), dtype=np.float32)\n",
    "\n",
    "- in your for loop\n",
    "X_train_gray[i] = normalize_img(element)\n",
    "\n",
    "https://discourse-cdn-sjc3.com/udacity/uploads/default/original/4X/5/7/1/5719666845aa31c56610cb2c27f4a16c7fc4c022.png\n",
    "3. converting RGB to YCbCr (Y: Luminance; Cb: Chrominance-Blue; and Cr: Chrominance-Red are the components. Luminance is very similar to the grayscale version of the original image)\n",
    "\n",
    "\n",
    "3.from image_preprocessor import ImagePreprocessor\n",
    "\n",
    "image_preprocessor = ImagePreprocessor()\n",
    "image_preprocessor.call() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    temp = []\n",
    "    for i in range(0,len(img)):\n",
    "        img = cv2.imread(img[i])\n",
    "        img = cv2.cvtColor(img[i],cv2.COLOR_BGR2GRAY)\n",
    "        img = np.reshape(img, img.shape + (1,))\n",
    "        temp.append(img)\n",
    "    #img = np.reshape(temp,(-1,32,32,1))\n",
    "    img = cv2.equalizeHist(img)\n",
    "    clahe = cv2.createCLAHE(cliplimit = 2.0, tileGridSize=(8,8))\n",
    "    img = np.sum(img/3,  keepdims = True)\n",
    "    img = img.astype(float) / 255.0\n",
    "    return img\n",
    "X_train= preprocess(X_train)\n",
    "X_test= preprocess(X_test)\n",
    "X_valid = preprocess(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply a sample image into one file\n",
    "sample = np.copy(X_train[0])\n",
    "sample = cv2.cvtColor(sample,cv2.COLOR_BGR2GRAY)\n",
    "#sample= np.expand_dims(cv2.equalizeHist(sample), axis = 2)\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform\n",
    "sample = X_train[0].copy()\n",
    "#sample = np.zeros((1,32,32,1))\n",
    "sample = cv2.cvtColor(sample,cv2.COLOR_BGR2GRAY)\n",
    "sample= np.expand_dims(cv2.equalizeHist(sample), axis = 2)\n",
    "#sample = np.reshape(len(sample),(32,32,1))\n",
    "#sample = cv2.equalizeHist(sample)\n",
    "clahe = cv2.createCLAHE(cliplimit = 2.0, tileGridSize=(8,8))\n",
    "sample = np.expand_dims(clahe.apply(sample), axis = 2)\n",
    "#sample = sample.astype(float) / 255.0\n",
    "#sample = np.sum(sample/3, axis=3, keepdims = True)\n",
    "#sample = np.reshape(sample,(-1,32,32,1))\n",
    "\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    return image.astype(float) / 255.0\n",
    "\n",
    "def preprocess(img):\n",
    "    image= [] \n",
    "    for i in img:\n",
    "        image.append(normalize(i))\n",
    "    return np.array(image).reshape((-1,32,32,1))\n",
    "\n",
    "\n",
    "#X_train= preprocess(X_train)\n",
    "#X_test= preprocess(X_test)\n",
    "#X_valid = preprocess(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    return img.astype(float)/255.0\n",
    "    \n",
    "def preprocess(img):\n",
    "    image = []\n",
    "    for i in img:\n",
    "        image.append(normalize(i))\n",
    "    return np.array(image).reshape((-1,32,32,1))\n",
    "\n",
    "def translation(img, x=1, y=1):\n",
    "    row,cols,_=img.shape\n",
    "    M = np.float32(([1,0,x],[0,1,y]))\n",
    "    return cv2,waroAffine(img,M(cols, rows)).astype()\n",
    "\n",
    "def rotation(img,angle= 10,scale=1):\n",
    "    rows,cols,_ = img.shape\n",
    "    martrix = cv2.getRotationMatrix2D((cols/2,rows/2),angle,scale)\n",
    "    return cv2.warpAffine(img,matrix,(cols, rows)).astype(np.unit8)\n",
    "\n",
    "def brightness_augment(img, value = 30):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    h,s,v = cv2.split(hsv)\n",
    "    lim = 255- value\n",
    "    v[v>lim]= 255\n",
    "    v[v<=lim] += value\n",
    "    hsv = cv2.merge([h,s,v])\n",
    "    img = cv2.cvtColor(hsv,cv2.COLOR_HSV2RGB)\n",
    "    return img\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "def augment_images(image, count = 1):\n",
    "    random_number = [1,2]\n",
    "    augment_choice = np.random.choice(random_number)\n",
    "    augment_choice = 2\n",
    "    \n",
    "    rotation_angle = int(10*(1+count/4))\n",
    "    brightness_value = int(30*(1+count/4))\n",
    "    \n",
    "    if augment_choice ==1:\n",
    "        image= rotation(image,rotation_angle)\n",
    "    else:\n",
    "        image = brightness_augment(image,brightness_value)\n",
    "    return image\n",
    "\n",
    "def balance_augment(X,Y, maximum_number = 1800, threshold = 1.0):\n",
    "    X_aug = []\n",
    "    Y_aug = []\n",
    "    unique_numbers, counts = np.unique(Y, return_counts = True)\n",
    "    for count, number in zip(counts, unique_numbers):\n",
    "        augment_number = maximum_number/count\n",
    "        if augment_number > threshold:\n",
    "            augment_number = int(augment_number)\n",
    "            for i in range(augment_number):\n",
    "                indices = (Y == number).nonzero()[0]\n",
    "                for j in indices:\n",
    "                    new_images = augment_images(X[j],i)\n",
    "                    X_aug.append(new_images)\n",
    "                    Y_aug.append(number)\n",
    "    X_aug = np.array(X_aug)\n",
    "    Y_aug = np.array(Y_aug)\n",
    "    print(X_aug.shape)\n",
    "    X = np.append(X,X_aug, axis = 0)\n",
    "    Y = np.append(Y,Y_aug, axis = 0)\n",
    "    return X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57837, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = balance_augment(X_train, y_train)\n",
    "\n",
    "#plt.imshow(preprocessed[0])\n",
    "#X_train = (X_train)\n",
    "#X_train.shape\n",
    "#balance_augment(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11fbc1240>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[56000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12f502908>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[56720])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 92636 into shape (32,32,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b0b0b4fc4477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3e2042596b3a>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 92636 into shape (32,32,1)"
     ]
    }
   ],
   "source": [
    "X_train= preprocess(X_train)\n",
    "y_train= preprocess(y_train)\n",
    "X_test= preprocess(X_test)\n",
    "X_valid = preprocess(X_valid)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Image Shape: ()\n",
      "updated image shapes two: ()\n"
     ]
    }
   ],
   "source": [
    "print(\"Updated Image Shape: {}\".format(y_train[0].shape))\n",
    "print(\"updated image shapes two: {}\".format(y_train[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train, y_train = balance_augment(X_train, y_train)\n",
    "\n",
    "plt.imshow(preprocessed[0])\n",
    "X_train = (X_train)\n",
    "X_train.shape\n",
    "balance_augment(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X Train before augmentation\".format(X_train.shape))\n",
    "print(\"X_train after\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_equalization(img_array):\n",
    "    l,w,h = img_array.shape[1:]\n",
    "    equalized_images = np.empty((0,l,w,h))\n",
    "    for i in range(len(img_array)):\n",
    "        img_yuv = cv2.cvtColor(img_array[i], cv2.COLOR_BGR2YUV)\n",
    "        img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "        img_out = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "        img_out = np.reshape(img_out, (1,l,w,h))\n",
    "        equalized_images = np.append(equalized_images, img_out,axis = 0)\n",
    "    return equalized_images \n",
    "\n",
    "#X_train = histogram_equalization(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 2\n",
    "Describe how you set up the training, validation and testing data for your model.\n",
    "Once I have my data preprocess, next splitting the data into 80,20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (90783, 32, 32, 3)\n",
      "X_test shape (12630, 32, 32, 3)\n",
      "Y_valid shape (1853, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#shuffle the data afterwards \n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.02, random_state=0)\n",
    "\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"Y_valid shape\", X_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#each time it will show a different image\n",
    "import random\n",
    "def showImg(data):\n",
    "    random = np.random.randint(len(data))\n",
    "    image = data[random].squeeze()\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    \n",
    "showImg(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (272349, 32, 32, 1)\n",
      "X_test shape (37890, 32, 32, 1)\n",
      "Y_valid shape (1853, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train= preprocess(X_train)\n",
    "X_test= preprocess(X_test)\n",
    "X_valid = preprocess(X_valid)\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"Y_valid shape\", X_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to tune the suggested algorithm have a 93.1% validation, and testing and training with a 100%, yet when new images from web are test on the classifer, it turns out having only 1 right out of 5. \n",
    "therefore, I am going to have more data by augmenting more for the classes that has lesser dataset on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.) For reference on how to build a deep neural network using TensorFlow, see Deep Neural Network in TensorFlow from the classroom\n",
    "\n",
    "\n",
    "I am using a 4 layer network :\n",
    "Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "Activation:relu\n",
    "Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "Layer 2: Convolutional. Output = 10x10x16.\n",
    "Activation: relu\n",
    "Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "Layer 3: Convolutional. Output = 1x1x400.\n",
    "Activation: relu\n",
    "Flatten. Input = 5x5x16. Output = 400\n",
    "Flatten x: Input = 1x1x400. Output = 400.\n",
    "Concat layer2flat and x. Input = 400 + 400. Output = 800\n",
    "Dropout\n",
    "Layer 3: Convolutional. Output = 1x1x400.\n",
    "Dropout\n",
    "Layer 4: Fully Connected. Input = 800. Output = 43.\n",
    "\n",
    "#Question 4\n",
    "How did you train your model?\n",
    " about the optimizer used, and some of the hyperparameters (learning rate, keep_prob).\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying leNet in Tensorflow\n",
    "\n",
    "from tensorflow.contrib.layers import flatten\n",
    "def LeNet(x):    \n",
    "    \n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    W1 = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma), name=\"W1\")\n",
    "    x = tf.nn.conv2d(x, W1, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b1 = tf.Variable(tf.zeros(6), name=\"b1\")\n",
    "    x = tf.nn.bias_add(x, b1)\n",
    "    print(\"layer 1 shape:\",x.get_shape())\n",
    "\n",
    "    # TODO: Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # TODO: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    layer1 = x\n",
    "    \n",
    "    # TODO: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    W2 = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma), name=\"W2\")\n",
    "    x = tf.nn.conv2d(x, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b2 = tf.Variable(tf.zeros(16), name=\"b2\")\n",
    "    x = tf.nn.bias_add(x, b2)\n",
    "                     \n",
    "    # TODO: Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    # TODO: Layer 3: Convolutional. Output = 1x1x128\n",
    "    W3 = tf.Variable(tf.truncated_normal(shape=(5, 5, 16, 128), mean = mu, stddev = sigma), name=\"W3\")\n",
    "    x = tf.nn.conv2d(x, W3, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b3 = tf.Variable(tf.zeros(128), name=\"b3\")\n",
    "    x = tf.nn.bias_add(x, b3)\n",
    "    \n",
    "    # TODO: Layer 4: Convolutional. Output = 2X2X400\n",
    "    W4 = tf.Variable(tf.truncated_normal(shape=(5, 5, 128, 400), mean = mu, stddev = sigma), name=\"W4\")\n",
    "    x = tf.nn.conv2d(x, W3, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b4 = tf.Variable(tf.zeros(400), name=\"b4\")\n",
    "    x = tf.nn.bias_add(x, b4)\n",
    "                     \n",
    "    # TODO: Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "    x = flatten(x)\n",
    "\n",
    "    # Dropout\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # TODO: Layer 5: Fully Connected. Input = 800. Output = 43.\n",
    "    W5 = tf.Variable(tf.truncated_normal(shape=(1600, 400), mean = mu, stddev = sigma), name=\"W5\")\n",
    "    b5 = tf.Variable(tf.zeros(400), name=\"b5\")    \n",
    "    x = tf.add(tf.matmul(x, W5), b5)\n",
    "    \n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    logits = tf.add(tf.matmul(x,W5),b5)\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(BATCH_SIZE, height, width, depth)\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "\n",
    "# Placeholder for labels\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "# One-hot encoding of labels\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "\n",
    "# Probability to keep units\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 shape: (?, 28, 28, 6)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 20 and 16 for 'Conv2D_22' (op: 'Conv2D') with input shapes: [?,13,13,20], [2,2,16,20].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    669\u001b[0m           \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors_as_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           status)\n\u001b[0m\u001b[1;32m    671\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 20 and 16 for 'Conv2D_22' (op: 'Conv2D') with input shapes: [?,13,13,20], [2,2,16,20].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-ccda65e1c7bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Pass input data to the LeNet function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Compare logits to the ground-truth labels and calculate the cross entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-c152957f4956>\u001b[0m in \u001b[0;36mLeNet\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# TODO: Layer 4: Convolutional. Output = 2X2X400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mW4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"W4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'VALID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mb4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    394\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    397\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    757\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    758\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2240\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1615\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 20 and 16 for 'Conv2D_22' (op: 'Conv2D') with input shapes: [?,13,13,20], [2,2,16,20]."
     ]
    }
   ],
   "source": [
    "# Learning rate\n",
    "rate = 0.001\n",
    "mu = 0 \n",
    "sigma = 0.1\n",
    "# Pass input data to the LeNet function\n",
    "logits = LeNet(x)\n",
    "\n",
    "# Compare logits to the ground-truth labels and calculate the cross entropy\n",
    "# Cross entopy is a measure how different the logits\n",
    "# are from the ground-truth training labels\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)\n",
    "# Average the cross entropy from all the training images\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "# Use Adam algorithm (alternative of stochastic gradient descent)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "# Optimizer uses backpropagation to update the network and minimize training loss\n",
    "#training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the logit prediction to the one hot encoded ground-truth label\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "# Calculate the model's overall accuracy by averaging the individual prediction accuracies\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict = {x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TensorFlow session and Initialize the variables\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training in progress...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        # Shuffle the training data to ensure that trainint isn't biased\n",
    "        # by the order of the images\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        # Break training data into batches and train the model on the each batch\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict = {x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        \n",
    "        # In the end of each EPOCH evaluate the model on validation data\n",
    "        training_accuracy = evaluate(X_train, y_train)\n",
    "        validation_accuracy = evaluate(X_valid, Y_valid)\n",
    "        print(\"EPOCH {0} ...\".format(i + 1))\n",
    "        print(\"Training Accuracy = {:.5f}\".format(training_accuracy))\n",
    "        print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
    "        print()\n",
    "    \n",
    "    # Save the model\n",
    "    try:\n",
    "        saver\n",
    "    except NameError:\n",
    "        saver = tf.train.Saver()\n",
    "    saver.save(sess, 'lenet_sign_classifier')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 5\n",
    "What approach did you take in coming up with a solution to this problem? It may have been a process of trial and error, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think this is suitable for the current problem.\n",
    "\n",
    "This has been a back and forth challenge where I truly invest my time on data agumentations vs trying other neural network and tuning their parameters. \n",
    "\n",
    "This takes me for a few weeks to get to this acceptable accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver = tf.train.Saver()\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)    \n",
    "    train_accuracy = evaluate(X_train, y_train)\n",
    "    print(\"Train Accuracy = {:.5f}\".format(train_accuracy))\n",
    "    valid_accuracy = evaluate(X_valid, Y_valid)\n",
    "    print(\"Validation Accuracy = {:.5f}\".format(valid_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    Test_accuracy = evaluate(X_train, y_train)\n",
    "    print(\"Test Accuracy = {:.5f}\".format(Test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 6\n",
    "Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult?\n",
    "\n",
    "Those images are search under German traffic sign and somehow it shouldnt be any harder or confusing than pervious training ones. I thought of using Hong Kong  traffic sign where there is an english and chinese character side by side but turns out it totally doesnt work at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "my_image = np.zeros((5,32,32,1))\n",
    "for i, img in enumerate(glob.glob('./traffic-signs-data/germansign/image*.jpg')):\n",
    "    image = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (32, 32))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = (image -128/128)\n",
    "    my_image[i][:][:][:] =np.array(image)[...,np.newaxis]\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    #figures[i] = my_image[i].squeeze()\n",
    "    #plot_figures(figures)\n",
    "labels = np.array([28,35,25,25,25], dtype = 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_single_item_array = []\n",
    "my_single_item_label_array = []\n",
    "\n",
    "for i in range(5):\n",
    "    my_single_item_array.append(my_image[i])\n",
    "    my_single_item_label_array.append(labels[i])\n",
    "\n",
    "    with tf.Session() as sess2:\n",
    "        sess2.run(tf.global_variables_initializer())\n",
    "#         saver = tf.train.import_meta_graph('./lenet.meta')\n",
    "        saver.restore(sess2, tf.train.latest_checkpoint('.'))\n",
    "        my_accuracy = evaluate(my_image, labels)\n",
    "        print('Image {}'.format(i+1))\n",
    "        print(\"Image Accuracy = {:.3f}\".format(my_accuracy))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess2, tf.train.latest_checkpoint('.'))\n",
    "    test_accuracy = evaluate(my_image, labels)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "\n",
    "    \n",
    "for i, image in enumerate(my_image):\n",
    "    #grid = plt.subplot(len(my_image)/4,5,i+1)\n",
    "    image = image.squeeze()\n",
    "    #grid.imshow(image), plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    #plt.title(signnames[str(outputclass[i])])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = LeNet(x)\n",
    "softmax = tf.nn.softmax(logits)\n",
    "\n",
    "sess2 = tf.get_default_session()\n",
    "\n",
    "with tf.Session() as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess2, tf.train.latest_checkpoint('.'))\n",
    "    test_accuracy = evaluate(my_image, labels)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    result = sess2.run(softmax, feed_dict={x: my_image,keep_prob: 1.})\n",
    "    values, indices = tf.nn.top_k(result, 5)\n",
    "    \n",
    "    predictions  = sess2.run(values)\n",
    "    predictionIndicies  = sess2.run(indices)\n",
    "    print(\"softmax probabilties for top 5 predictions are:\")\n",
    "    print(predictions)\n",
    "    print(\"predictionIndicies\")\n",
    "    print(predictionIndicies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = LeNet(x)\n",
    "softmax = tf.nn.softmax(logits)\n",
    "\n",
    "sess2 = tf.get_default_session()\n",
    "\n",
    "with tf.Session() as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess2, tf.train.latest_checkpoint('.'))\n",
    "    test_accuracy = evaluate(my_image, labels)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    result = sess2.run(softmax, feed_dict={x: my_image,keep_prob: 1.})\n",
    "    values, indices = tf.nn.top_k(result, 5)\n",
    "    \n",
    "    predictions  = sess2.run(values)\n",
    "    predictionIndicies  = sess2.run(indices)\n",
    "    print(\"softmax probabilties for top 5 predictions are:\")\n",
    "    print(predictions)\n",
    "    print(\"predictionIndicies\")\n",
    "    print(predictionIndicies)\n",
    "    \n",
    "top_k_values = predictions[0]\n",
    "top_k_indices = predictionIndicies[1]\n",
    "\n",
    "ind = np.arange(5) \n",
    "\n",
    "for i in range(5):\n",
    "    plt.figure(figsize=(4,2))\n",
    "    values = top_k_values[i]\n",
    "    plt.bar(ind, values, 0.2, color='b')\n",
    "    plt.ylabel('Softmax probability')\n",
    "    plt.xlabel('Labels')\n",
    "    plt.title('Top 5 Softmax Probabilities for Test Image {}'.format(str(i+1)))\n",
    "    plt.xticks(ind, tuple(top_k_indices[i]))\n",
    "\n",
    "plt.show()\n",
    "    #bar(np.arange(n_classes), result[(i-1)//2]) \n",
    "    #plt.figure()\n",
    "    #plt.bar(result.indices[i],result.values[i], align='center', alpha=0.5)\n",
    "    #plt.xlim([0,42])\n",
    "    #plt.title(\"{}, label = {}\".format( names[i], Y_test_web[i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/ashoeb81/CarND-TrafficSigns-Solution-Final/blob/master/Traffic_Signs_Recognition.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

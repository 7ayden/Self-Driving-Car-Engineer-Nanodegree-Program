{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer vision projects on classifying traffic sign. \n",
    "\n",
    "\n",
    ": data augmentation, why this CNN and plot the probability of the knn top5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From exporation, there are 34799 training sets and 12630 testing sets. \n",
    "Each of the image has 32*32*3.\n",
    "Roughly around 40+ classes.\n",
    "The goal is to leverage a unbalance and relativity speaking small dataset to classify with deep neuarl networking.\n",
    "\n",
    "Recall from deep learning foundations course from deeplearning.ai, classic machine learning requires around 100-10,000 samples and deep learning requires around 1 million sample. \n",
    "\n",
    "Given neural network does not require any data balance having each class, I would like to see the difference between data split and also data size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (34799, 32, 32, 3)\n",
      "X_test shape (12630, 32, 32, 3)\n",
      "Y_valid shape (12630, 32, 32, 3)\n",
      "Y_valid shape (12630,)\n",
      "Y_train shape (34799, 32, 32, 3)\n",
      "Y_test shape (12630, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "training_file = \"./traffic-signs-data/train.p\"\n",
    "validation_file = \"./traffic-signs-data/test.p\"\n",
    "testing_file = \"./traffic-signs-data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "    \n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, Y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"Y_valid shape\", X_valid.shape)\n",
    "print(\"Y_valid shape\", Y_valid.shape)\n",
    "print(\"Y_train shape\", X_train.shape)\n",
    "print(\"Y_test shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signnames = pd.read_csv('signnames.csv')\n",
    "label_names = list(signnames)\n",
    "y_train_df = pd.DataFrame()\n",
    "y_train_df['label'] = y_train\n",
    "\n",
    "# Get current size\n",
    "figsize=(15, 7)\n",
    " \n",
    "# Prints: [8.0, 6.0]\n",
    "\n",
    "item, count = np.unique(y_train, return_counts=True)\n",
    "freq = np.array((item, count)).T\n",
    "plt.figure(11)\n",
    "plt.yticks(range(len(y_train)), signnames.SignName)\n",
    "#plt.yticks(list(map(lambda x: label_dict[x], y_train['label'].value_counts().index.tolist())))            \n",
    "plt.barh(item, count, alpha=0.3)\n",
    "plt.title('Traffic Sign Data Distrubition')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot and see how data distribution looks like\n",
    "n_classes = len(np.unique(y_train))\n",
    "Y_test_class = len(np.unique(y_test))\n",
    "\n",
    "def plot_dist(data) :\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.hist(y_testing, np.arange(-0.5, n_classes+1.5), stacked=True)\n",
    "    (Y_test_class, np.arange(n_classes+1.5)).plt.bar()\n",
    "    #ax.hist(y_testing, np.arange(-0.5, n_classes+1.5), stacked=True)\n",
    "    #ax1.hist(y_training, np.arange(-0.5, n_classes+1.5), stacked=True)\n",
    "    #pd.concat(dict(df1= y_testing, df2 = y_training), azis = 1).plot(kind='bar', stack =True)\n",
    "    ax.set_xlim(-0.5,n_classes-0.5)\n",
    "    ax.legend()\n",
    "    ax.set_title('Distribution of images')\n",
    "\n",
    "plot_dist(y_train)\n",
    "plot_dist(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classID = signnames['SignName'].values\n",
    "n_classes = np.unique(y_train[0]).shape[0]\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "for i, y in enumerate(y_train):\n",
    "    ax.bar(range(n_classes), np.bincount(y), width=1., align='center', color=colors[i], label=labels[i])\n",
    "\n",
    "plt.xlim([-1, n_classes])\n",
    "ax.xaxis.grid(False)\n",
    "ax.xaxis.set_ticks_position('none')\n",
    "ax.yaxis.grid(False)\n",
    "ax.set_xticks(range(n_classes))\n",
    "ax.set_xticklabels(class_table, rotation=90, size='xx-small')\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "legend_frame = legend.get_frame()\n",
    "legend_frame.set_facecolor('white')\n",
    "legend_frame.set_edgecolor('black')\n",
    "plt.ylabel('# examples')\n",
    "plt.title(title)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the exploration, there the training and testing dataset obtains about the same ratios of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#Question 1\n",
    "Describe how you preprocessed the data. Why did you choose that technique?\n",
    "\n",
    "1. I am planning to preprocess my image by converting into greyscale that can save a lot of computer power from 3 channels to 1, since I dont have GPU on my laptop nor planning to train on cloud. \n",
    "Next step is to normalizied dataset, the process would allows each dimensions have a similar scale\n",
    "\n",
    "\n",
    "there are always different approches on this: \n",
    "1. loop image one by one with CV2 (easiest way)\n",
    "2. passing through normalizing (pixel - 128)/ 128, then img.astype(np.float32), passing through the CV2, and finally reshape from 32,32,3 to 32,32,1\n",
    "X_train_gray = np.zeros((N, 32, 32, 1), dtype=np.float32)\n",
    "\n",
    "- in your for loop\n",
    "X_train_gray[i] = normalize_img(element)\n",
    "\n",
    "https://discourse-cdn-sjc3.com/udacity/uploads/default/original/4X/5/7/1/5719666845aa31c56610cb2c27f4a16c7fc4c022.png\n",
    "3. converting RGB to YCbCr (Y: Luminance; Cb: Chrominance-Blue; and Cr: Chrominance-Red are the components. Luminance is very similar to the grayscale version of the original image)\n",
    "\n",
    "\n",
    "3.from image_preprocessor import ImagePreprocessor\n",
    "\n",
    "image_preprocessor = ImagePreprocessor()\n",
    "image_preprocessor.call() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    temp = []\n",
    "    for i in range(0,len(img)):\n",
    "        img = cv2.imread(img[i])\n",
    "        img = cv2.cvtColor(img[i],cv2.COLOR_BGR2GRAY)\n",
    "        img = np.reshape(img, img.shape + (1,))\n",
    "        temp.append(img)\n",
    "    #img = np.reshape(temp,(-1,32,32,1))\n",
    "    img = cv2.equalizeHist(img)\n",
    "    clahe = cv2.createCLAHE(cliplimit = 2.0, tileGridSize=(8,8))\n",
    "    img = np.sum(img/3,  keepdims = True)\n",
    "    img = img.astype(float) / 255.0\n",
    "    return img\n",
    "X_train= preprocess(X_train)\n",
    "X_test= preprocess(X_test)\n",
    "X_valid = preprocess(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply a sample image into one file\n",
    "sample = np.copy(X_train[0])\n",
    "sample = cv2.cvtColor(sample,cv2.COLOR_BGR2GRAY)\n",
    "#sample= np.expand_dims(cv2.equalizeHist(sample), axis = 2)\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform\n",
    "sample = X_train[0].copy()\n",
    "#sample = np.zeros((1,32,32,1))\n",
    "sample = cv2.cvtColor(sample,cv2.COLOR_BGR2GRAY)\n",
    "sample= np.expand_dims(cv2.equalizeHist(sample), axis = 2)\n",
    "#sample = np.reshape(len(sample),(32,32,1))\n",
    "#sample = cv2.equalizeHist(sample)\n",
    "clahe = cv2.createCLAHE(cliplimit = 2.0, tileGridSize=(8,8))\n",
    "sample = np.expand_dims(clahe.apply(sample), axis = 2)\n",
    "#sample = sample.astype(float) / 255.0\n",
    "#sample = np.sum(sample/3, axis=3, keepdims = True)\n",
    "#sample = np.reshape(sample,(-1,32,32,1))\n",
    "\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    return image.astype(float) / 255.0\n",
    "\n",
    "def preprocess(img):\n",
    "    image= [] \n",
    "    for i in range(0,len(img)):\n",
    "        img = cv2.imread(img[i])\n",
    "        img = cv2.cvtColor(img[i],cv2.COLOR_BGR2GRAY)\n",
    "        image.append(normalize(img[i]))\n",
    "    return np.array(image).reshape((-1,32,32,1))\n",
    "\n",
    "\n",
    "X_train= preprocess(X_train)\n",
    "X_test= preprocess(X_test)\n",
    "X_valid = preprocess(X_valid)\n",
    "y_train= preprocess(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    return image.astype(float) / 255.0\n",
    "\n",
    "def preprocess(img):\n",
    "    image= [] \n",
    "    for i in img:\n",
    "        i = np.dot(i[...,:3], [0.299, 0.587, 0.114])\n",
    "        image.append(normalize(i))\n",
    "    return np.array(image).reshape((-1,32,32,1))\n",
    "\n",
    "\n",
    "X_train= preprocess(X_train)\n",
    "X_test= preprocess(X_test)\n",
    "X_valid = preprocess(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92636,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1392ab198>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = X_train\n",
    "random = np.random.randint(len(data))\n",
    "plt.imshow(data[random].squeeze(), cmap=\"gray\")\n",
    "#plt.imshow(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation(img, x=1, y=1):\n",
    "    row,cols,_=img.shape\n",
    "    M = np.float32(([1,0,x],[0,1,y]))\n",
    "    return cv2,waroAffine(img,M(cols, rows)).astype()\n",
    "\n",
    "def rotation(img,angle= 10,scale=1):\n",
    "    rows,cols,_ = img.shape\n",
    "    martrix = cv2.getRotationMatrix2D((cols/2,rows/2),angle,scale)\n",
    "    return cv2.warpAffine(img,matrix,(cols, rows)).astype(np.unit8)\n",
    "\n",
    "def brightness_augment(img, value = 30):\n",
    "\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    h,s,v = cv2.split(hsv)\n",
    "    lim = 255- value\n",
    "    v[v>lim]= 255\n",
    "    v[v<=lim] += value\n",
    "    hsv = cv2.merge([h,s,v])\n",
    "    img = cv2.cvtColor(hsv,cv2.COLOR_HSV2RGB)\n",
    "    return img\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "def augment_images(image, count = 1):\n",
    "    random_number = [1,2]\n",
    "    augment_choice = np.random.choice(random_number)\n",
    "    augment_choice = 2\n",
    "    \n",
    "    rotation_angle = int(10*(1+count/4))\n",
    "    brightness_value = int(30*(1+count/4))\n",
    "    \n",
    "    if augment_choice ==1:\n",
    "        image= rotation(image,rotation_angle)\n",
    "    else:\n",
    "        image = brightness_augment(image,brightness_value)\n",
    "    return image\n",
    "\n",
    "def balance_augment(X,Y, maximum_number = 1800, threshold = 1.0):\n",
    "    X_aug = []\n",
    "    Y_aug = []\n",
    "    unique_numbers, counts = np.unique(Y, return_counts = True)\n",
    "    for count, number in zip(counts, unique_numbers):\n",
    "        augment_number = maximum_number/count\n",
    "        if augment_number > threshold:\n",
    "            augment_number = int(augment_number)\n",
    "            for i in range(augment_number):\n",
    "                indices = (Y == number).nonzero()[0]\n",
    "                for j in indices:\n",
    "                    new_images = augment_images(X[j],i)\n",
    "                    X_aug.append(new_images)\n",
    "                    Y_aug.append(number)\n",
    "    X_aug = np.array(X_aug)\n",
    "    Y_aug = np.array(Y_aug)\n",
    "    print(X_aug.shape)\n",
    "    X = np.append(X,X_aug, axis = 0)\n",
    "    Y = np.append(Y,Y_aug, axis = 0)\n",
    "    return X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57837, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = balance_augment(X_train, y_train)\n",
    "\n",
    "#plt.imshow(preprocessed[0])\n",
    "#X_train = (X_train)\n",
    "#X_train.shape\n",
    "#balance_augment(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[56720])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= preprocess(X_train)\n",
    "y_train= preprocess(y_train)\n",
    "X_test= preprocess(X_test)\n",
    "X_valid = preprocess(X_valid)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Updated Image Shape: {}\".format(y_train[0].shape))\n",
    "print(\"updated image shapes two: {}\".format(y_train[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train, y_train = balance_augment(X_train, y_train)\n",
    "\n",
    "plt.imshow(preprocessed[0])\n",
    "X_train = (X_train)\n",
    "X_train.shape\n",
    "balance_augment(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X Train before augmentation\".format(X_train.shape))\n",
    "print(\"X_train after\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_equalization(img_array):\n",
    "    l,w,h = img_array.shape[1:]\n",
    "    equalized_images = np.empty((0,l,w,h))\n",
    "    for i in range(len(img_array)):\n",
    "        img_yuv = cv2.cvtColor(img_array[i], cv2.COLOR_BGR2YUV)\n",
    "        img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "        img_out = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "        img_out = np.reshape(img_out, (1,l,w,h))\n",
    "        equalized_images = np.append(equalized_images, img_out,axis = 0)\n",
    "    return equalized_images \n",
    "\n",
    "#X_train = histogram_equalization(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 2\n",
    "Describe how you set up the training, validation and testing data for your model.\n",
    "Once I have my data preprocess, next splitting the data into 80,20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (90783, 32, 32, 1)\n",
      "X_test shape (12630, 32, 32, 1)\n",
      "Y_valid shape (1853, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "#shuffle the data afterwards \n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.02, random_state=0)\n",
    "\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"Y_valid shape\", X_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#each time it will show a different image\n",
    "import random\n",
    "def showImg(data):\n",
    "    random = np.random.randint(len(data))\n",
    "    image = data[random].squeeze()\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    \n",
    "showImg(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (90783, 32, 32, 1)\n",
      "X_test shape (12630, 32, 32, 1)\n",
      "Y_valid shape (1853, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "#X_train= preprocess(X_train)\n",
    "#X_test= preprocess(X_test)\n",
    "#X_valid = preprocess(X_valid)\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"Y_valid shape\", X_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to tune the suggested algorithm have a 93.1% validation, and testing and training with a 100%, yet when new images from web are test on the classifer, it turns out having only 1 right out of 5. \n",
    "therefore, I am going to have more data by augmenting more for the classes that has lesser dataset on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.) For reference on how to build a deep neural network using TensorFlow, see Deep Neural Network in TensorFlow from the classroom\n",
    "\n",
    "\n",
    "I am using a 4 layer network :\n",
    "Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "Activation:relu\n",
    "Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "Layer 2: Convolutional. Output = 10x10x16.\n",
    "Activation: relu\n",
    "Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "Layer 3: Convolutional. Output = 1x1x400.\n",
    "Activation: relu\n",
    "Flatten. Input = 5x5x16. Output = 400\n",
    "Flatten x: Input = 1x1x400. Output = 400.\n",
    "Concat layer2flat and x. Input = 400 + 400. Output = 800\n",
    "Dropout\n",
    "Layer 3: Convolutional. Output = 1x1x400.\n",
    "Dropout\n",
    "Layer 4: Fully Connected. Input = 800. Output = 43.\n",
    "\n",
    "#Question 4\n",
    "How did you train your model?\n",
    " about the optimizer used, and some of the hyperparameters (learning rate, keep_prob).\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet_5(x):\n",
    " \n",
    "     # Layer 1 : Convolutional Layer. Input = 32x32x1, Output = 28x28x1.\n",
    "    conv1_w = tf.Variable(tf.truncated_normal(shape = [5,5,1,6],mean = 0, stddev = 0.1))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1 = tf.nn.conv2d(x,conv1_w, strides = [1,1,1,1], padding='VALID') + conv1_b \n",
    "     # TODO: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "     # Pooling Layer. Input = 28x28x1. Output = 14x14x6.\n",
    "    pool_1 = tf.nn.max_pool(conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding='VALID')\n",
    "\n",
    "\n",
    "     # TODO: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_w = tf.Variable(tf.truncated_normal(shape = [5,5,6,16], mean = 0, stddev = 0.1))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2 = tf.nn.conv2d(pool_1, conv2_w, strides = [1,1,1,1], padding='VALID') + conv2_b\n",
    "     # TODO: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    # TODO: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    pool_2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding='VALID')\n",
    "\n",
    "\n",
    "     # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc1 = flatten(pool_2)\n",
    "\n",
    "\n",
    "     # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_w = tf.Variable(tf.truncated_normal(shape = (400,120), mean = 0, stddev = 0.1))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1 = tf.matmul(fc1,fc1_w) + fc1_b\n",
    "\n",
    "     # TODO: Activation.\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "     # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_w = tf.Variable(tf.truncated_normal(shape = (120,84), mean = 0, stddev = 0.1))\n",
    "    fc2_b = tf.Variable(tf.zeros(84))\n",
    "    fc2 = tf.matmul(fc1,fc2_w) + fc2_b\n",
    "     # TODO: Activation.\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "\n",
    "     # TODO: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    fc3_w = tf.Variable(tf.truncated_normal(shape = (84,10), mean = 0 , stddev = 0.1))\n",
    "    fc3_b = tf.Variable(tf.zeros(10))\n",
    "    logits = tf.matmul(fc2, fc3_w) + fc3_b\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None,32,32,1])\n",
    "y_ = tf.placeholder(tf.int32, (None))\n",
    "#Invoke LeNet function by passing features\n",
    "logits = LeNet_5(x)\n",
    "#Softmax with cost function implementation\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y_,1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# Evaluate function\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y_: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... with dataset -  90783\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n\t [[Node: Placeholder_7 = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Placeholder_7', defined at:\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-22-aa8d332546ca>\", line 5, in <module>\n    y = tf.placeholder(tf.int32, (None))\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1587, in placeholder\n    name=name)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2043, in _placeholder\n    name=name)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n\t [[Node: Placeholder_7 = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n\t [[Node: Placeholder_7 = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d24adfc262e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n\t [[Node: Placeholder_7 = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Placeholder_7', defined at:\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-22-aa8d332546ca>\", line 5, in <module>\n    y = tf.placeholder(tf.int32, (None))\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1587, in placeholder\n    name=name)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2043, in _placeholder\n    name=name)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/jaydenmilton/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n\t [[Node: Placeholder_7 = Placeholder[dtype=DT_INT32, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training... with dataset - \", num_examples)\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y_: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, '/tmp/lenet.ckpt')\n",
    "    print(\"Model saved %s \"%save_path)\n",
    "    \n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying leNet in Tensorflow\n",
    "\n",
    "from tensorflow.contrib.layers import flatten\n",
    "def LeNet(x):    \n",
    "    \n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    W1 = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma), name=\"W1\")\n",
    "    x = tf.nn.conv2d(x, W1, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b1 = tf.Variable(tf.zeros(6), name=\"b1\")\n",
    "    x = tf.nn.bias_add(x, b1)\n",
    "    print(\"layer 1 shape:\",x.get_shape())\n",
    "\n",
    "    # TODO: Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # TODO: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    layer1 = x\n",
    "    \n",
    "    # TODO: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    W2 = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma), name=\"W2\")\n",
    "    x = tf.nn.conv2d(x, W2, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b2 = tf.Variable(tf.zeros(16), name=\"b2\")\n",
    "    x = tf.nn.bias_add(x, b2)\n",
    "                     \n",
    "    # TODO: Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    # TODO: Layer 3: Convolutional. Output = 1x1x128\n",
    "    W3 = tf.Variable(tf.truncated_normal(shape=(5, 5, 16, 128), mean = mu, stddev = sigma), name=\"W3\")\n",
    "    x = tf.nn.conv2d(x, W3, strides=[1, 2, 2, 1], padding='VALID')\n",
    "    b3 = tf.Variable(tf.zeros(128), name=\"b3\")\n",
    "    x = tf.nn.bias_add(x, b3)\n",
    "    \n",
    "    # TODO: Layer 4: Convolutional. Output = 2X2X400\n",
    "    W4 = tf.Variable(tf.truncated_normal(shape=(5, 5, 128, 400), mean = mu, stddev = sigma), name=\"W4\")\n",
    "    x = tf.nn.conv2d(x, W3, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    b4 = tf.Variable(tf.zeros(400), name=\"b4\")\n",
    "    x = tf.nn.bias_add(x, b4)\n",
    "                     \n",
    "    # TODO: Activation.\n",
    "    x = tf.nn.relu(x)\n",
    "    x = flatten(x)\n",
    "\n",
    "    # Dropout\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # TODO: Layer 5: Fully Connected. Input = 800. Output = 43.\n",
    "    W5 = tf.Variable(tf.truncated_normal(shape=(1600, 400), mean = mu, stddev = sigma), name=\"W5\")\n",
    "    b5 = tf.Variable(tf.zeros(400), name=\"b5\")    \n",
    "    x = tf.add(tf.matmul(x, W5), b5)\n",
    "    \n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    \n",
    "    W6 = tf.Variable(tf.truncated_normal(shape=(400, 43), mean = mu, stddev = sigma), name=\"W5\")\n",
    "    b6 = tf.Variable(tf.zeros(43), name=\"b5\")      \n",
    "    logits = tf.add(tf.matmul(x,W6),b6)\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(BATCH_SIZE, height, width, depth)\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "\n",
    "# Placeholder for labels\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "# One-hot encoding of labels\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "\n",
    "# Probability to keep units\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 shape: (?, 28, 28, 6)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 128 and 16 for 'Conv2D_19' (op: 'Conv2D') with input shapes: [?,3,3,128], [5,5,16,128].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    669\u001b[0m           \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors_as_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           status)\n\u001b[0m\u001b[1;32m    671\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 128 and 16 for 'Conv2D_19' (op: 'Conv2D') with input shapes: [?,3,3,128], [5,5,16,128].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ccda65e1c7bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Pass input data to the LeNet function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Compare logits to the ground-truth labels and calculate the cross entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-903085161c7d>\u001b[0m in \u001b[0;36mLeNet\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# TODO: Layer 4: Convolutional. Output = 2X2X400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mW4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"W4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'VALID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mb4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    394\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    397\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    757\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    758\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2240\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1615\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 128 and 16 for 'Conv2D_19' (op: 'Conv2D') with input shapes: [?,3,3,128], [5,5,16,128]."
     ]
    }
   ],
   "source": [
    "# Learning rate\n",
    "rate = 0.001\n",
    "mu = 0 \n",
    "sigma = 0.1\n",
    "# Pass input data to the LeNet function\n",
    "logits = LeNet(x)\n",
    "# Create the TensorFlow session and Initialize the variables\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training in progress...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        # Shuffle the training data to ensure that trainint isn't biased\n",
    "        # by the order of the images\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        # Break training data into batches and train the model on the each batch\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict = {x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        \n",
    "        # In the end of each EPOCH evaluate the model on validation data\n",
    "        training_accuracy = evaluate(X_train, y_train)\n",
    "        validation_accuracy = evaluate(X_valid, Y_valid)\n",
    "        print(\"EPOCH {0} ...\".format(i + 1))\n",
    "        print(\"Training Accuracy = {:.5f}\".format(training_accuracy))\n",
    "        print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
    "        print()\n",
    "    \n",
    "    # Save the model\n",
    "    try:\n",
    "        saver\n",
    "    except NameError:\n",
    "        saver = tf.train.Saver()\n",
    "    saver.save(sess, 'lenet_sign_classifier')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the logit prediction to the one hot encoded ground-truth label\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "# Calculate the model's overall accuracy by averaging the individual prediction accuracies\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict = {x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TensorFlow session and Initialize the variables\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training in progress...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        # Shuffle the training data to ensure that trainint isn't biased\n",
    "        # by the order of the images\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        # Break training data into batches and train the model on the each batch\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict = {x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        \n",
    "        # In the end of each EPOCH evaluate the model on validation data\n",
    "        training_accuracy = evaluate(X_train, y_train)\n",
    "        validation_accuracy = evaluate(X_valid, Y_valid)\n",
    "        print(\"EPOCH {0} ...\".format(i + 1))\n",
    "        print(\"Training Accuracy = {:.5f}\".format(training_accuracy))\n",
    "        print(\"Validation Accuracy = {:.5f}\".format(validation_accuracy))\n",
    "        print()\n",
    "    \n",
    "    # Save the model\n",
    "    try:\n",
    "        saver\n",
    "    except NameError:\n",
    "        saver = tf.train.Saver()\n",
    "    saver.save(sess, 'lenet_sign_classifier')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 5\n",
    "What approach did you take in coming up with a solution to this problem? It may have been a process of trial and error, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think this is suitable for the current problem.\n",
    "\n",
    "This has been a back and forth challenge where I truly invest my time on data agumentations vs trying other neural network and tuning their parameters. \n",
    "\n",
    "This takes me for a few weeks to get to this acceptable accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver = tf.train.Saver()\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)    \n",
    "    train_accuracy = evaluate(X_train, y_train)\n",
    "    print(\"Train Accuracy = {:.5f}\".format(train_accuracy))\n",
    "    valid_accuracy = evaluate(X_valid, Y_valid)\n",
    "    print(\"Validation Accuracy = {:.5f}\".format(valid_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    Test_accuracy = evaluate(X_train, y_train)\n",
    "    print(\"Test Accuracy = {:.5f}\".format(Test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 6\n",
    "Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult?\n",
    "\n",
    "Those images are search under German traffic sign and somehow it shouldnt be any harder or confusing than pervious training ones. I thought of using Hong Kong  traffic sign where there is an english and chinese character side by side but turns out it totally doesnt work at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "my_image = np.zeros((5,32,32,1))\n",
    "for i, img in enumerate(glob.glob('./traffic-signs-data/germansign/image*.jpg')):\n",
    "    image = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (32, 32))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = (image -128/128)\n",
    "    my_image[i][:][:][:] =np.array(image)[...,np.newaxis]\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    #figures[i] = my_image[i].squeeze()\n",
    "    #plot_figures(figures)\n",
    "labels = np.array([28,35,25,25,25], dtype = 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_single_item_array = []\n",
    "my_single_item_label_array = []\n",
    "\n",
    "for i in range(5):\n",
    "    my_single_item_array.append(my_image[i])\n",
    "    my_single_item_label_array.append(labels[i])\n",
    "\n",
    "    with tf.Session() as sess2:\n",
    "        sess2.run(tf.global_variables_initializer())\n",
    "#         saver = tf.train.import_meta_graph('./lenet.meta')\n",
    "        saver.restore(sess2, tf.train.latest_checkpoint('.'))\n",
    "        my_accuracy = evaluate(my_image, labels)\n",
    "        print('Image {}'.format(i+1))\n",
    "        print(\"Image Accuracy = {:.3f}\".format(my_accuracy))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess2, tf.train.latest_checkpoint('.'))\n",
    "    test_accuracy = evaluate(my_image, labels)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "\n",
    "    \n",
    "for i, image in enumerate(my_image):\n",
    "    #grid = plt.subplot(len(my_image)/4,5,i+1)\n",
    "    image = image.squeeze()\n",
    "    #grid.imshow(image), plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    #plt.title(signnames[str(outputclass[i])])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = LeNet(x)\n",
    "softmax = tf.nn.softmax(logits)\n",
    "\n",
    "sess2 = tf.get_default_session()\n",
    "\n",
    "with tf.Session() as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess2, tf.train.latest_checkpoint('.'))\n",
    "    test_accuracy = evaluate(my_image, labels)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    result = sess2.run(softmax, feed_dict={x: my_image,keep_prob: 1.})\n",
    "    values, indices = tf.nn.top_k(result, 5)\n",
    "    \n",
    "    predictions  = sess2.run(values)\n",
    "    predictionIndicies  = sess2.run(indices)\n",
    "    print(\"softmax probabilties for top 5 predictions are:\")\n",
    "    print(predictions)\n",
    "    print(\"predictionIndicies\")\n",
    "    print(predictionIndicies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = LeNet(x)\n",
    "softmax = tf.nn.softmax(logits)\n",
    "\n",
    "sess2 = tf.get_default_session()\n",
    "\n",
    "with tf.Session() as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess2, tf.train.latest_checkpoint('.'))\n",
    "    test_accuracy = evaluate(my_image, labels)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    result = sess2.run(softmax, feed_dict={x: my_image,keep_prob: 1.})\n",
    "    values, indices = tf.nn.top_k(result, 5)\n",
    "    \n",
    "    predictions  = sess2.run(values)\n",
    "    predictionIndicies  = sess2.run(indices)\n",
    "    print(\"softmax probabilties for top 5 predictions are:\")\n",
    "    print(predictions)\n",
    "    print(\"predictionIndicies\")\n",
    "    print(predictionIndicies)\n",
    "    \n",
    "top_k_values = predictions[0]\n",
    "top_k_indices = predictionIndicies[1]\n",
    "\n",
    "ind = np.arange(5) \n",
    "\n",
    "for i in range(5):\n",
    "    plt.figure(figsize=(4,2))\n",
    "    values = top_k_values[i]\n",
    "    plt.bar(ind, values, 0.2, color='b')\n",
    "    plt.ylabel('Softmax probability')\n",
    "    plt.xlabel('Labels')\n",
    "    plt.title('Top 5 Softmax Probabilities for Test Image {}'.format(str(i+1)))\n",
    "    plt.xticks(ind, tuple(top_k_indices[i]))\n",
    "\n",
    "plt.show()\n",
    "    #bar(np.arange(n_classes), result[(i-1)//2]) \n",
    "    #plt.figure()\n",
    "    #plt.bar(result.indices[i],result.values[i], align='center', alpha=0.5)\n",
    "    #plt.xlim([0,42])\n",
    "    #plt.title(\"{}, label = {}\".format( names[i], Y_test_web[i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/ashoeb81/CarND-TrafficSigns-Solution-Final/blob/master/Traffic_Signs_Recognition.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
